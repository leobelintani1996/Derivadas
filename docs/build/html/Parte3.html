<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
    <meta charset="utf-8" />
        <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
        <title>3. Diferenciação Automática &mdash; Página Inicial</title>
    
    <link rel="stylesheet" type="text/css" href="_static/dist/fontawesome.css" />
      <link rel="stylesheet" type="text/css" href="_static/dist/theme.css?v=f85e6685" />
            <link rel="index" title="Index" href="genindex.html" />
            <link rel="search" title="Search" href="search.html" />
            <link rel="top" title="Página Inicial" href="#" />
            <link rel="next" title="4. Referências" href="Parte4.html" />
            <link rel="prev" title="2. Diferenciação Numérica" href="Parte2.html" />
    </head>
<body>
    <script type="text/javascript" src="_static/dist/blocking.js"></script>
    <header class="container-fluid bg-primary">
        <a class="btn btn-sm btn-light skip-to-content-link" href="#main">Skip to content</a>
        <div class="container-fluid">
            <div class="navbar navbar-expand-lg navbar-dark font-weight-bold">
                    <a href="index.html"
                        
                        class="logo navbar-brand"
                    >
                        <img src="_static/Logomarca_UFSCAR.svg" width="150" height="150"
                            class="logo-img"
                        />
                        TCC - Leonardo Belintani
                    </a>
                
                
                <button class="navbar-toggler btn btn-primary d-lg-none" type="button" data-toggle="collapse" data-target="#collapseSidebar" aria-expanded="false" aria-controls="collapseExample">
                    <span class="navbar-toggler-icon"></span>
                    <span class="sr-only">menu</span>
                </button>
            </div>
        </div>
    </header>
    <div class="container-fluid">
        <div class="row">
            <aside class="col-12 col-lg-3 sidebar-container">
                <div id="collapseSidebar" class="collapse sticky-top d-lg-block pt-5 pr-lg-4">
<div id="searchbox" class="searchbox mb-6 px-1" role="search">
    <form id="search-form" action="search.html" autocomplete="off" method="get" role="search">
        <div class="input-group">
            <div class="input-group-prepend">
                <div class="input-group-text border-right-0 bg-white py-3 pl-3 pr-2"><span class="fas fa-search"></span></div>
            </div>
            <input class="form-control py-3 pr-3 pl-1 h-100 border-left-0" type="search" name="q" placeholder="Search documentation" aria-label="Search documentation" id="searchinput" />
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div><div class="site-toc">
    <nav class="toc mt-3" aria-label="Main menu">
        <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Parte1.html">1. <strong>Derivadas analíticas e definições</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Parte2.html">2. <strong>Diferenciação Numérica</strong></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. <strong>Diferenciação Automática</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Parte4.html">4. <strong>Referências</strong></a></li>
</ul>

    </nav>
    <template data-toggle-item-template>
        <button class="btn btn-sm btn-link toctree-expand" type="button">
            <span class="sr-only">Toggle menu contents</span>
        </button>
    </template>
</div>
                    <div class="d-lg-none border-bottom">
                        
                    </div>
                </div>
            </aside>
            <div class="col-12 col-lg-9 pt-5">
                <header class="row align-items-baseline">
                    <div class="col">
                        <nav aria-label="breadcrumb">
    <ol class="breadcrumb m-0 p-0 bg-transparent">
        <li class="breadcrumb-item"><a href="index.html">Docs</a></li>
        <li class="breadcrumb-item active" aria-current="page">3. <strong>Diferenciação Automática</strong></li>
    </ol>
</nav>
                    </div>
                    <div class="col-sm-12 col-lg-auto mt-3 mt-lg-3">
                        <noscript>
                            <p>JavaScript is required to toggle light/dark mode..</p>
                        </noscript>
                        <button id="wagtail-theme" class="btn btn-sm btn-light text-decoration-none" type="button">
                            <span class="dark-only"><i class="fas fa-sun"></i> Light mode</span>
                            <span class="light-only"><i class="fas fa-moon"></i> Dark mode</span>
                        </button>
    <a class="btn btn-sm btn-light text-decoration-none" href="https://github.com/leobelintani1996/DerivadasParte3.rst" rel="nofollow">
        <span class="btn-icon"><span class="fab fa-github"></span></span>
        <span class="btn-text">Edit on GitHub</span>
    </a>
    <a class="btn btn-sm btn-light text-decoration-none" href="_sources/Parte3.rst.txt" rel="nofollow">
        <span class="btn-icon"><span class="fas fa-code"></span></span>
        <span class="btn-text">View source</span>
    </a>
                        
                    </div>
                </header>
                <div class="row" >
                    <div class="col-12">
                        <hr class="w-100 my-4">
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-lg-9 order-last order-lg-first rst-content">
                        <main role="main" id="main">
    <section id="diferenciacao-automatica">
<h1>3. <strong>Diferenciação Automática</strong><a class="headerlink" href="#diferenciacao-automatica" title="Link to this heading">¶</a></h1>
<section id="motivacao">
<h2>3.1. <strong>Motivação</strong><a class="headerlink" href="#motivacao" title="Link to this heading">¶</a></h2>
<p>Como já vimos nos capítulos anteriores, podemos classificar os tipos de derivações em 2 grupos:</p>
<ol class="arabic simple">
<li><p>Diferenciação analítica ou simbólica.</p></li>
<li><p>Diferenciação numérica.</p></li>
</ol>
<p>Uma terceira opção surge ao estudarmos o método da diferenciação automática.</p>
<p>A diferenciação analítica nos permite derivar uma determinada função e ao final da operação encontrar a forma fechada da derivada, ou seja, uma expressão
matemática que vai simbolizar a derivada da função. Podemos calcular a derivada em um determinado ponto simplesmente ao substituirmos o ponto na expressão.
Já a derivação numérica vai nos fornecer uma aproximação da derivada em um ponto, não nos fornecendo uma forma fechada da derivada, ou seja, estamos estimando o limite da função em um determinado ponto quando nosso passo
<span class="math notranslate nohighlight">\(h\)</span> tende a um valor estabelecido por nós.</p>
<p>De acordo com <a class="reference internal" href="Parte4.html#ref3"><span class="std std-ref">Baydin [2018]</span></a>, a diferenciação automática não é um tipo de técnica simbólica nem numérica. Ela faz parte de um outro grupo de métodos de diferenciação, chamada de diferenciação automática ou autodiferenciação (termo mais utilizado na área de machine learning).
A diferenciação automática baseia-se na decomposição de funções em operações básicas, cujas derivadas são conhecidas. Essas podem ser combinadas em um algoritmo para derivar a função em um determinado ponto. No resultado final vamos obter um valor
numérico da derivada em um determinado ponto, nunca uma forma fechada da derivada, como a diferenciação analítica. Por não se basear em cálculos utilizando diferenças finitas, a diferenciação automática não possui erros de aproximação significativos, melhorando  muito a precisão do cálculo executado, contudo ainda
temos os erros de arredondamento presentes, visto que estes são inerentes aos cálculos computacionais.</p>
<p>Esse método facilita otimizações e análises avançadas em sistemas complexos, particularmente em redes neurais artificiais, onde a precisão e eficiência no cálculo de gradientes são essenciais.
A base teórica por trás de todo o método se baseia em o que chamamos de números duais.</p>
</section>
<section id="numeros-duais">
<h2>3.2. <strong>Números duais</strong><a class="headerlink" href="#numeros-duais" title="Link to this heading">¶</a></h2>
<p>O conceito por trás da diferenciação automática parte da definição dos números duais. Através dos números duais podemos escrever o valor de uma função em um ponto e sua derivada no ponto.</p>
<p>Os números duais são uma extensão dos números reais, similares aos números complexos, contudo possuem suas particularidades.
Os números complexos, por exemplo, são expressos no seguinte formato:</p>
<div class="admonition-definicao admonition">
<p class="admonition-title">Definição</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;z = x + yi \tag{73} \\ \\
\end{align}\end{split}\]</div>
<p>Onde <span class="math notranslate nohighlight">\(x\)</span> e <span class="math notranslate nohighlight">\(y\)</span> são números reais e representam respectivamente as coordenadas  <span class="math notranslate nohighlight">\((x,y)\)</span> de um ponto no espaço  e <span class="math notranslate nohighlight">\(i\)</span> é o que chamamos de unidade imaginária e tem em sua definição <span class="math notranslate nohighlight">\(i=\sqrt{-1}\)</span> .</p>
<p>Os números duais são definidos da seguinte forma</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;a = a + a'd \tag{74} \\ \\
\end{align}\end{split}\]</div>
<p>Onde <span class="math notranslate nohighlight">\(a\)</span> é uma função avaliada em um determinado ponto, <span class="math notranslate nohighlight">\(a'\)</span> é a derivada da função avaliada no ponto e <span class="math notranslate nohighlight">\(d\)</span> é a unidade dual e é definida como <span class="math notranslate nohighlight">\(d^2 = 0\)</span> .</p>
<p>As regras aritméticas que regem o cálculo envolvendo números duais podem ser definidas como:</p>
<p>Dado o número dual <span class="math notranslate nohighlight">\(a = (a+a'd)\)</span> e o número dual <span class="math notranslate nohighlight">\(b=(b+b'd)\)</span> , temos</p>
<p>Adição</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;a + b = (a + a'd) + (b+b'd) = (a+b) + d(a'+b') \tag{75} \\ \\
\end{align}\end{split}\]</div>
<p>Subtração</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;a - b = (a + a'd) - (b+b'd) = (a-b) + d(a'-b') \tag{76} \\ \\
\end{align}\end{split}\]</div>
<p>Multiplicação</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;a \times b = (a+a'd) \times (b+b'd) = (ab) + (ab'd) + (a'bd) + (a'b'd^2) = (ab) + d(ab' + a'b) \tag{77} \\ \\
\end{align}\end{split}\]</div>
<p>Podemos notar que a primeira componente se comporta como a função avaliada no ponto e a segunda componente se comporta como a derivada
da função em um determinado ponto.</p>
</div>
<p>A essa altura podemos passar a escrever os números duais e suas operações em um formato de pares ordenados de números reais,
deixando a variável dual <span class="math notranslate nohighlight">\(d\)</span> de lado, uma vez que ela serve apenas para nos guiar a respeito de qual parcela da equação é responsável por representar a derivada de uma função
em um determinado ponto.</p>
<p>Logo podemos escrever</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;a = (a,a') \tag{78} \\ \\
\end{align}\end{split}\]</div>
<div class="admonition-regra-de-derivacao-para-numeros-duais admonition">
<p class="admonition-title">Regra de derivação para números duais</p>
<p>Por fim, podemos sintetizar as operações mais básicas envolvendo os números duais como</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(a + b = (a+b, a' + b')\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(a - b = (a-b, a' - b')\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(a \times b = (ab, a'b + ab')\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(a / b = (\frac{a}{b}, \frac{a'b - ab'}{b^2} )\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(c = (c,0)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(x = (x,1)\)</span></p></td>
</tr>
</tbody>
</table>
<p>Repare que, o valor de uma variável constante avaliada em um determinado ponto é a própria variável, e o valor de sua derivada é zero – como vimos no primeiro capítulo deste material.
Para o caso da variável independente, a mesma lógica é utilizada.</p>
</div>
<div class="admonition-exemplo admonition">
<p class="admonition-title">Exemplo</p>
<p>Calcule a derivada da função <span class="math notranslate nohighlight">\(f(x) = \frac{(x+2)(x-3)}{(x-4)}\)</span> no ponto <span class="math notranslate nohighlight">\(x = 6\)</span>.</p>
<p>O primeiro passo a se fazer é escrever a função e realizar os cálculos através das definições básicas apresentadas acima.</p>
<p>Temos então</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;F(f(x),f'(x)) = (x+2) \times (x-3) / (x-4) = \\ \\
&amp;F(f(x),f'(x)) = {[(x,1) + (2,0)] \times [(x,1) - (3,0)]} / {[(x,1) - (4,0)]} \\ \\
\end{align}\end{split}\]</div>
<p>Repare que buscamos encontrar <span class="math notranslate nohighlight">\(f'(6)\)</span> .</p>
<p>Logo, podemos substituir da seguinte forma</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;F(f(6),f'(6)) = {[(6,1) + (2,0)] \times [(6,1) - (3,0)]} / {[(6,1) - (4,0)]} = \\ \\
&amp;F(f(6),f'(6)) = {[(8,1)] \times [(3,1)]} / {[(2,1)]} = \\ \\
&amp;F(f(6),f'(6)) = {[(8 \times 3, 1 \times 3 + 1 \times 8)]} / {[(2,1)]} = \\ \\
&amp;F(f(6),f'(6)) = {[(24, 11)]} / {[(2,1)]} = \\ \\
&amp;F(f(6),f'(6)) = {[24/2, (11 \times 2 - 24 \times 1)/2^2]} = \\ \\
&amp;F(f(6),f'(6)) = {[12,(22 - 24)/4]} = \\ \\
&amp;F(f(6),f'(6)) = {[12,-1/2]}
\end{align}\end{split}\]</div>
<p>Que em sua essência nos diz a função e sua derivada no ponto <span class="math notranslate nohighlight">\(x=6\)</span> . Logo, podemos concluir que <span class="math notranslate nohighlight">\(F(6,1) = {[f(6) = 12, f'(6)=-1/2]}\)</span> .</p>
</div>
<p>Podemos ainda definir as regras de derivação por números duais para outras funções conhecidas, além de definirmos a regra da cadeia.</p>
<div class="admonition-regra-da-cadeia-e-funcoes-conhecidas admonition">
<p class="admonition-title">Regra da cadeia e funções conhecidas</p>
<p>Iremos definir abaixo a regra da cadeia assim como a derivada de outras funções conhecidas através do uso dos números duais.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Polinômios: <span class="math notranslate nohighlight">\(p(a) = ((p(a), a'p'(a)))\)</span></p></td>
<td><p>Seno: <span class="math notranslate nohighlight">\(sen(a) = sen((a,a'))=(sen(a),a'cos(a))\)</span></p></td>
</tr>
<tr class="row-even"><td><p>Cosseno: <span class="math notranslate nohighlight">\(cos(a) = cos((a,a'))=(cos(a),-a'sen(a))\)</span></p></td>
<td><p>Exponencial: <span class="math notranslate nohighlight">\(e^{(a)} = e^{((a,a'))}=(e^{a},a'e^{a})\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Logaritmo: <span class="math notranslate nohighlight">\(ln(a) = ln((a,a'))=(ln(a),\frac{a'}{a})\)</span></p></td>
<td><p>Raiz quadrada: <span class="math notranslate nohighlight">\(\sqrt{a}= \sqrt{((a,a'))}=(\sqrt{a},\frac{a'}{2\sqrt{a}})\)</span></p></td>
</tr>
</tbody>
</table>
<p>Repare que a regra da cadeia apresentada no capítulo 1 deste material, já está embutida nas expressões acima, mas podemos a definir como</p>
<div class="math notranslate nohighlight">
\[\begin{align}
&amp;g(a) = g((a,a'))=(g(a,a'g'(a)))
\end{align}\]</div>
</div>
<p>Podemos, por fim, resolver um exemplo um pouco mais elaborado afim de fixarmos as definições apresentadas acima.</p>
<div class="admonition-exemplo admonition">
<p class="admonition-title">Exemplo</p>
<p>Dada a função a seguir <span class="math notranslate nohighlight">\(f(x) = e^{sen(2x)}\)</span>, encontre sua derivada no ponto <span class="math notranslate nohighlight">\(x=0.5\)</span> utilizando os números duais.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;F(f(x), f'(x)) = e^{sen((2,0) \times (x,1))} = \\ \\
&amp;F(f(0.5),f'(0.5)) = e^{sen((2,0) \times (0.5,1))} = \\ \\
&amp;F(f(0.5),f'(0.5)) = e^{sen(2 \times 0.5, 0 \times 0.5 + 2 \times 1)} = \\ \\
&amp;F(f(0.5),f'(0.5)) = e^{sen(1, 2)} = \\ \\
&amp;F(f(0.5),f'(0.5)) = (e^{sen(1)}, 2cos(1)e^{sen(1)}) = \\ \\
&amp;F(f(0.5),f'(0.5)) \approx [(2.3197768, 2.5067615)]
\end{align}\end{split}\]</div>
</div>
<p>Podemos ainda extrapolar o cálculo de uma variável praticado até o presente momento justificado por fins didáticos. Vamos considerar uma função de várias variáveis do tipo <span class="math notranslate nohighlight">\(f(x,y,z)\)</span> e, ao aplicarmos as mesmas regras apresentadas, obtemos os mesmos resultados discutidos.</p>
<p>A diferença é que não estamos mais calculando derivadas totais sob as parcelas de cada eixo, e sim derivadas parciais. Isso se torna útil em um contexto voltado ao machine learning, onde muitas vezes vamos calcular gradientes de funções, que podem ser definido como:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;\vec{\nabla} f(x,y,z) = \frac{\partial}{\partial x}f(x,y,z) \hat{x} + \frac{\partial}{\partial y}f(x,y,z) \hat{y} + \frac{\partial}{\partial z}f(x,y,z) \hat{z} \tag{79} \\ \\
\end{align}\end{split}\]</div>
<p>Por fim, a derivação via números duais ainda não é de fato a diferenciação automática em sua essência. Para diferenciarmos a função em um determinado ponto automaticamente precisamos de fato automatizar o processo.
É nítido que para expressões complexas o esforço matemático (para diferenciar uma determinada função utilizando os números duais) vai ser algo extremamente custoso, logo, visamos automatizar o processo de modo que um algoritmo em Python seja capaz de realizar as tarefas apresentadas acima sem grandes problemas.</p>
<p>Na seção apresentada a seguir iremos criar nosso algoritmo de diferenciação automática.</p>
</section>
<section id="implementando-a-autodiferenciacao">
<h2>3.3. <strong>Implementando a autodiferenciação</strong><a class="headerlink" href="#implementando-a-autodiferenciacao" title="Link to this heading">¶</a></h2>
<p>Podemos implementar um código em Python de forma que as operações das derivadas via números duais possam ser de fato automatizadas.
A ideia central é “quebrar” uma determinada função matemática em várias partes cuja derivada possa ser escrita com as regras apresentadas acima.</p>
<p>O código apresentado carrega consigo alguns conceitos um pouco mais avançados dentro da área da programação, como é o caso do uso de classes, objetos e sobrecarga de operadores (operators overloading).</p>
<p>Uma tentativa de didatizar o conteúdo é através de comentários no corpo do código e de uma breve explicação ao final. De qualquer forma, não se preocupe caso tenha alguma dúvida. O objetivo deste material não é ensinar programação de fato.</p>
<p>Por fim, podemos apresentar o algoritmo abaixo, onde a autodiferenciação foi implementada de fato.</p>
<p>O primeiro exemplo se trata de calcularmos a derivada da função <span class="math notranslate nohighlight">\(f(x) = x^5 + 1\)</span> no ponto <span class="math notranslate nohighlight">\(x = 1\)</span></p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>  <span class="c1"># Importa o módulo math para acessar funções matemáticas.</span>

<span class="k">class</span> <span class="nc">Dif</span><span class="p">:</span>
<span class="c1"># Classe Dif para representar e operar com variáveis diferenciais.</span>
<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>  <span class="c1"># Valor primal: valor da função no ponto de interesse.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">d</span>  <span class="c1"># Primeira derivada: derivada da função no ponto de interesse.</span>

<span class="c1"># Sobrecarrega o operador de adição.</span>
<span class="k">def</span> <span class="fm">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Soma os valores primais e as derivadas se &#39;other&#39; for uma instância de Dif.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Soma um número real ao valor primal se &#39;other&#39; for um número.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">+</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de adição para permitir adição comutativa (número + objeto Dif).</span>
<span class="k">def</span> <span class="fm">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Soma os valores primais e as derivadas se &#39;other&#39; for uma instância de Dif.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">+</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Soma um número real ao valor primal se &#39;other&#39; for um número.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">+</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de multiplicação.</span>
<span class="k">def</span> <span class="fm">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Aplica a regra do produto para multiplicação.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Multiplica um número real pelo valor primal e pela derivada.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de multiplicação para permitir multiplicação comutativa.</span>
<span class="k">def</span> <span class="fm">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Aplica a regra do produto para multiplicação.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Multiplica um número real pelo valor primal e pela derivada.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de subtração.</span>
<span class="k">def</span> <span class="fm">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Subtrai os valores primais e as derivadas se &#39;other&#39; for uma instância de Dif.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Subtrai um número real do valor primal.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de subtração para permitir subtração comutativa (número - objeto Dif).</span>
<span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Subtrai os valores primais e as derivadas se &#39;other&#39; for uma instância de Dif.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">-</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Subtrai um número real do valor primal.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de divisão.</span>
<span class="k">def</span> <span class="fm">__truediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Aplica a regra da divisão para a divisão de Difs.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">p</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Divide um número real pelo valor primal e pela derivada.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">/</span> <span class="n">other</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de divisão para permitir divisão comutativa (número / objeto Dif).</span>
<span class="k">def</span> <span class="fm">__rtruediv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">Dif</span><span class="p">):</span>
        <span class="c1"># Aplica a regra da divisão para a divisão de Difs.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">p</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">other</span><span class="o">.</span><span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">other</span><span class="o">.</span><span class="n">p</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Divide um número real pelo valor primal e pela derivada.</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">/</span> <span class="n">other</span><span class="p">)</span>

<span class="c1"># Sobrecarrega o operador de potência &#39;**&#39;.</span>
<span class="k">def</span> <span class="fm">__pow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="c1"># Verifica se o expoente é um inteiro.</span>
        <span class="c1"># Calcula x^other, onde &#39;x&#39; é o valor primal do objeto Dif e &#39;other&#39; é o expoente.</span>
        <span class="n">new_primal</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">**</span> <span class="n">other</span>
        <span class="n">new_derivative</span> <span class="o">=</span> <span class="n">other</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">**</span> <span class="p">(</span><span class="n">other</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span>
        <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="n">new_primal</span><span class="p">,</span> <span class="n">new_derivative</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Lança um erro se o expoente não for um inteiro.</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Potência só suportada para expoentes inteiros.&quot;</span><span class="p">)</span>


<span class="c1"># Funções auxiliares para criar uma variável diferencial como uma constante ou uma variável.</span>
<span class="k">def</span> <span class="nf">constante</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># Retorna um objeto Dif com derivada 0.</span>

<span class="k">def</span> <span class="nf">variavel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Dif</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Retorna um objeto Dif com derivada 1.</span>

<span class="c1"># Função de exemplo que aceita múltiplas variáveis diferenciais.</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Calcula a função f(x) = (x**5 + 1)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Ponto de interesse x =1 .</span>
<span class="n">x</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Exibe a derivada da função f no ponto de interesse.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;df/dx f(</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">Dif</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id1">
<img alt="_images/image_19.png" src="_images/image_19.png" />
<figcaption>
<p><span class="caption-text">Figura 19</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Repare que a classe Dif é projetada para trabalhar com variáveis diferenciais e realizar operações aritméticas básicas, mantendo o controle do valor da função (valor primal) e sua derivada em um ponto específico.</p>
<p>A classe “Dif” representa uma variável diferencial. Cada instância possui dois atributos:</p>
<ul class="simple">
<li><p>p: Valor primal, que é o valor da função no ponto de interesse.</p></li>
<li><p>d: Derivada da função no ponto de interesse.</p></li>
</ul>
<p>A classe sobrecarrega vários operadores aritméticos para permitir operações entre objetos Dif ou entre um objeto Dif e um número real:</p>
<ul class="simple">
<li><p>__add__ e __radd__: Adição comutativa.</p></li>
<li><p>__sub__ e __rsub__: Subtração comutativa.</p></li>
<li><p>__mul__ e __rmul__: Multiplicação comutativa.</p></li>
<li><p>__truediv__ e __rtruediv__: Divisão comutativa.</p></li>
<li><p>__pow__: Potência (apenas para expoentes inteiros).</p></li>
</ul>
<p>Essas operações são realizadas de maneira que tanto o valor primal quanto a derivada possam ser corretamente calculadas seguindo as regras do cálculo diferencial apresentados nas seções acima.</p>
<p>Métodos Auxiliares - constante e variável:</p>
<ul class="simple">
<li><p>Constante(a): Cria uma variável diferencial que representa uma constante (derivada zero).</p></li>
<li><p>Variável(x): Cria uma variável diferencial que representa uma variável independente (derivada um).</p></li>
</ul>
<p>Por fim, o código define uma função <span class="math notranslate nohighlight">\(f(x)\)</span> que calcula <span class="math notranslate nohighlight">\(f(x) = (x^5 + 1)\)</span> . Em seguida, cria objetos Dif para <span class="math notranslate nohighlight">\(x\)</span>  com
valores específicos e calcula o valor da função <span class="math notranslate nohighlight">\(f\)</span>  e sua derivada nesses pontos. Finalmente, imprime o valor da função (resultado.p) e o valor da derivada (resultado.d) para os valores dados de <span class="math notranslate nohighlight">\(x\)</span> .</p>
<p>Podemos ainda calcular a derivada simbólica da função afim de compararmos o resultado obtido. É importante entendermos que nem sempre isso será possível. Caso a função seja complexa demais, podemos utilizar como métrica a própria derivada numérica, apresentada no capítulo 2 deste material.</p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sympy</span> <span class="kn">import</span> <span class="n">symbols</span><span class="p">,</span> <span class="n">diff</span>

<span class="c1"># Definir a variável simbólica</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">symbols</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="c1"># Definir a função f(x)</span>
<span class="n">f</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Calcular a derivada simbólica de f(x)</span>
<span class="n">f_prime</span> <span class="o">=</span> <span class="n">diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Avaliar a derivada em x = 1</span>
<span class="n">f_prime_val</span> <span class="o">=</span> <span class="n">f_prime</span><span class="o">.</span><span class="n">subs</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">evalf</span><span class="p">()</span>

<span class="c1"># Imprimir o resultado</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;f&#39;(1) =&quot;</span><span class="p">,</span> <span class="n">f_prime_val</span><span class="p">)</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id2">
<img alt="_images/image_20.png" src="_images/image_20.png" />
<figcaption>
<p><span class="caption-text">Figura 20</span><a class="headerlink" href="#id2" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Podemos observar que o resultado obtido com o uso do nosso algoritmo para diferenciar automaticamente a função é exatamente o mesmo que a derivação simbólica nos fornece, nos mostrando que, de fato, ao calcularmos derivadas em pontos específicos, a autodiferenciação surge como uma forte alternativa para essa tarefa.</p>
<p>Vamos ainda resolver outro exemplo, onde a função a ser derivada é uma função de duas variáveis, no formato <span class="math notranslate nohighlight">\(f(x,y) = y(x^5 + 1)\)</span> , onde buscamos encontrar o gradiente da função, ou seja, <span class="math notranslate nohighlight">\(\vec{\nabla} f(x,y) = \frac{\partial }{\partial x} f(x,y) \hat{x} + \frac{\partial }{\partial y} f(x,y) \hat{y}\)</span> nos pontos <span class="math notranslate nohighlight">\(x = 1\)</span> e <span class="math notranslate nohighlight">\(y = 3\)</span>.</p>
<p>O algoritmo apresentado abaixo segue o mesmo processo do exemplo anterior, contudo estamos considerando mais variáveis.</p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="o">...</span><span class="p">]</span>

<span class="c1"># Função de exemplo que aceita múltiplas variáveis diferenciais.</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Calcula a função f(x, y) = y(x**5 + 1)</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Ponto de interesse x = 1 e y = 3.</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span>

<span class="c1"># Exibe a derivada da função f no ponto de interesse.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;df/dx f(</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">Dif</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">Dif</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;df/dy f(</span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2">,</span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="s2">) = </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">Dif</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">Dif</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">d</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id3">
<img alt="_images/image_21.png" src="_images/image_21.png" />
<figcaption>
<p><span class="caption-text">Figura 21</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>É nítido que nosso algoritmo é uma implementação simplificada. Você pode notar que não definimos funções auxiliares como seno, cosseno, tangente, exponencial ou logaritmo, por exemplo, logo, estamos limitados dentro das possibilidades de funções existentes.</p>
<p>Isso foi feito como uma medida de simplificar o algoritmo em si, tornando-o mais didático e menos denso. Outro ponto interessante é que para derivadas de ordens superiores a implementação não é tão simples, logo,
é necessário o uso de bibliotecas especializadas em diferenciação automática.</p>
<p>Dentro do grande universo da linguagem Python, existem bibliotecas que diferenciam automaticamente funções, onde estes algoritmos já estão implementados. Na seção 3.5 iremos falar melhor sobre uma das bibliotecas utilizadas: a biblioteca JAX.</p>
<p>Vamos ainda discutir os dois principais modos de diferenciação automática. A diferenciação automática apresentada acima é conhecida como modo direto,
na seção abaixo iremos entender a diferença entre os modos direto e reverso e quais suas consequências.</p>
</section>
<section id="modo-direto-e-reverso">
<h2>3.4. <strong>Modo direto e reverso</strong><a class="headerlink" href="#modo-direto-e-reverso" title="Link to this heading">¶</a></h2>
<p>Existem dois modos principais de diferenciação automática: o modo direto (forward accumulation) e o modo reverso (reverse accumulation).</p>
<p><strong>Modo Direto (Forward Accumulation)</strong>:</p>
<p>Neste modo, a diferenciação é realizada da parte interna para a parte externa da função. Isso significa que as derivadas são calculadas seguindo a ordem das operações como elas aparecem no algoritmo. Por exemplo, se uma função é composta como
<span class="math notranslate nohighlight">\(f(g(h(x)))\)</span>, a diferenciação começa com <span class="math notranslate nohighlight">\(h(x)\)</span> , seguida por  <span class="math notranslate nohighlight">\(g(h(x))\)</span>, e finalmente <span class="math notranslate nohighlight">\(f(g(h(x)))\)</span>. Este modo é eficiente quando há um pequeno número de variáveis independentes em relação às quais as derivadas são calculadas,
pois para cada variável independente é necessário uma aplicação do algoritmo.</p>
<p><strong>Modo Reverso (Reverse Accumulation)</strong>:</p>
<p>Este modo funciona de maneira oposta ao modo direto. A diferenciação é realizada da parte externa para a parte interna. Primeiro, calcula-se a derivada da função externa e, em seguida, propaga-se essa informação para as funções internas. Usando o mesmo exemplo de
<span class="math notranslate nohighlight">\(f(g(h(x)))\)</span>, começaríamos com a derivada de <span class="math notranslate nohighlight">\(f\)</span>, seguida por <span class="math notranslate nohighlight">\(g\)</span> e <span class="math notranslate nohighlight">\(h\)</span>. O modo reverso é particularmente eficiente quando há muitas variáveis independentes, pois permite calcular as derivadas em relação a todas essas variáveis em apenas uma aplicação do algoritmo.</p>
<p>Em resumo, a escolha entre o modo direto e o modo reverso depende da estrutura da função e do número de variáveis independentes. O modo direto é mais eficiente para funções com poucas variáveis independentes, enquanto o modo reverso é mais adequado para funções com muitas variáveis independentes.</p>
<p>A grande maioria das bibliotecas de diferenciação automática escolhem por conta própria (automaticamente) se o modo utilizado será o direto ou o reverso, logo, não é estritamente necessário um estudo rigoroso sobre os dois modos para se utilizar as biblioteca em si. Contudo, caso o leitor queira se aventurar, sugiro a leitura das referências <a class="reference internal" href="Parte4.html#ref4"><span class="std std-ref">4 e 5</span></a> que tratam de forma mais aprofundada a implementação do modo reverso e direto.</p>
</section>
<section id="bibliotecas-de-diferenciacao-automatica">
<h2>3.5. <strong>Bibliotecas de diferenciação automática</strong><a class="headerlink" href="#bibliotecas-de-diferenciacao-automatica" title="Link to this heading">¶</a></h2>
<p>Como já discutido, a diferenciação automática é uma técnica crucial em aprendizado de máquina, especialmente em redes neurais artificiais,
permitindo o cálculo eficiente de gradientes e derivadas.</p>
<p>Podemos citar três bibliotecas populares que implementam essa técnica. São elas: TensorFlow, PyTorch e JAX. Cada qual possui características únicas que as tornam adequadas para diferentes tipos de tarefas e abordagens. O objetivo aqui é apresentar a sintaxe da biblioteca JAX, além
de discorrer um pouco sobre as funcionalidades da mesma.</p>
<section id="jax">
<h3>3.5.1. <strong>JAX</strong><a class="headerlink" href="#jax" title="Link to this heading">¶</a></h3>
<p>A biblioteca JAX, desenvolvida pela Google Research, é uma biblioteca que combina NumPy, Auto Diferenciação e aceleração de hardware.
O que a torna especial é sua capacidade de transformar funções Python puras em funções que podem ser aceleradas em CPUs, GPUs e TPUs.</p>
<p>A biblioteca particularmente trabalha analisando o código da função Python e o converte em um formato intermediário que pode ser otimizado.
Durante essa conversão, ela aplica várias transformações, como fusão de operações, eliminação de operações redundantes e paralelização.</p>
<p>Após a transformação, ela utiliza o XLA (Accelerated Linear Algebra) para compilar este formato intermediário em código de máquina de alto desempenho. O XLA é um compilador avançado que otimiza o código para execução específica em CPUs, GPUs ou TPUs.
Essa compilação é feita de forma Just-In-Time (JIT), ou seja, ocorre em tempo de execução, permitindo que a JAX otimize o código com base no contexto específico em que está sendo executado.
O resultado final é uma versão da função original que pode ser executada muito mais rapidamente do que o código Python puro. Isso pode ser bastante útil para operações com muitos cálculos, como em machine learning e em processamento de grandes conjuntos de dados e simulações.
O XLA e a compilação JIT são particularmente úteis quando a função é executada em um hardware especializado, como GPUs e TPUs, que são projetados para lidar eficientemente com operações de alta intensidade computacional.</p>
<p>Para o usuário final, isso significa que é possível escrever funções em Python, uma linguagem de alto nível e fácil de usar, e ainda assim aproveitar o desempenho de baixo nível que normalmente requereria programação em uma linguagem mais complexa e de baixo nível, como C, por exemplo.
Além disso, essa abordagem permite que os cientistas de dados e pesquisadores se concentrem mais na modelagem e na lógica do problema, sem se preocuparem tanto com os detalhes de otimização de desempenho.</p>
<p>Abaixo iremos apresentar 2 exemplos do funcionamento da biblioteca e como se dá sua sintaxe no contexto da diferenciação automática. Primeiro vamos resolver o caso em que
queremos calcular <span class="math notranslate nohighlight">\(f'(x)\)</span> , onde <span class="math notranslate nohighlight">\(f(x) = x(x+1)\)</span> no ponto <span class="math notranslate nohighlight">\(x=1\)</span> .</p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="c1">#Define a função que queremos utilizar</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Calculando o gradiente da função</span>
<span class="n">grad_f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="c1"># Ponto em que o gradiente será calculado</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span>

<span class="c1"># Calculando o gradiente no ponto (x=1)</span>
<span class="n">grad_no_ponto</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1">#Mostra o resultado do gradiente no ponto.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;O gradiente de f no ponto (x=1) é:&quot;</span><span class="p">,</span> <span class="n">grad_no_ponto</span><span class="p">)</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id4">
<img alt="_images/image_22.png" src="_images/image_22.png" />
<figcaption>
<p><span class="caption-text">Figura 22</span><a class="headerlink" href="#id4" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Repare que para o usuário final o que de fato importa é a sintaxe</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">grad_f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<p>onde todo o resto do código está em Python puro, ou seja, a biblioteca JAX é utilizada neste caso, única
e exclusivamente para se calcular o gradiente da função fornecida.</p>
<p>Um segundo exemplo pode ser apresentado, onde a ideia é calcular o gradiente de uma função de duas ou três variáveis. No caso apresentado, vamos calcular o gradiente da função <span class="math notranslate nohighlight">\(f(x,y) = x(x+y^2)\)</span> nos pontos <span class="math notranslate nohighlight">\(x = 1; y= 2\)</span> .</p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#Importa as bibliotecas</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>


<span class="c1">#Define a função</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Calculando o gradiente da função</span>
<span class="n">grad_f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Ponto em que o gradiente será calculado</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">2.0</span>

<span class="c1"># Calculando o gradiente no ponto (x=1, y=2)</span>
<span class="n">grad_no_ponto</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1">#Mostra o resultado do gradiente no ponto.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;O gradiente de f no ponto (x=1, y=2) é:&quot;</span><span class="p">,</span> <span class="n">grad_no_ponto</span><span class="p">)</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id5">
<img alt="_images/image_23.png" src="_images/image_23.png" />
<figcaption>
<p><span class="caption-text">Figura 23</span><a class="headerlink" href="#id5" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>A diferença ao calcularmos o gradiente de funções de uma, duas ou até quantas variáveis quisermos, vai ser evidente na linha em que chamamos a função:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>Se estivermos tratando de 3 variáveis por exemplo, utilizaríamos</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">argnums</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<p>Uma vez que entendemos como a técnica da diferenciação automática funciona e como podemos utilizar a biblioteca JAX para o cálculo de gradientes, podemos de fato mostrar suas aplicações dentro da área do machine learning, onde eu e meus orientadores (Daniel Silva e João Teles), durante a minha graduação utilizamos destas ferramentas para resolver alguns conhecidos problemas da Física.</p>
<p>A próxima sessão surge com a ideia de tocarmos de forma suave na definição de redes neurais artificiais e de algumas propriedades que a tangem, como:
função custo, taxa de aprendizagem, pesos sinápticos, bias, etc,  e então apresentar um exemplo menos sofisticado, onde vamos de fato poder enxergar o potencial desta
poderosa técnica que está intimamente relacionada com a diferenciação automática.</p>
</section>
</section>
<section id="aplicacao-em-redes-neurais-artificiais">
<h2>3.6. <strong>Aplicação em Redes Neurais Artificiais</strong><a class="headerlink" href="#aplicacao-em-redes-neurais-artificiais" title="Link to this heading">¶</a></h2>
<p>Redes Neurais Artificiais (RNAs) são sistemas computacionais inspirados no funcionamento do cérebro humano, projetados para aprender e processar informações de maneira análoga
aos seres humanos. Estas redes são formadas por unidades de processamento chamadas neurônios artificiais, que estão interconectados e trabalham em conjunto para resolver problemas
específicos.</p>
<p>O treinamento de uma RNA é guiado por uma função custo (ou função de perda - <span class="math notranslate nohighlight">\(L(w,b)\)</span> ), que avalia o desempenho da rede. Esta função custo vai depender do sistema em questão que queremos treinar e otimizar, mas ela sempre vai
depender no mínimo de parâmetros conhecidos como peso sináptico e bias.</p>
<p>O objetivo do treinamento é minimizar a função custo, nos indicando que a rede está aprendendo efetivamente. O algoritmo de gradiente descendente é utilizado para
minimizar a função custo. Ele ajusta iterativamente os pesos <span class="math notranslate nohighlight">\(w\)</span> e o bias <span class="math notranslate nohighlight">\(b\)</span> da rede na direção que reduz o erro.
O processo de atualização em cada iteração é dado por:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
&amp;w_{novo} = w_{antigo} -\alpha \frac{\partial }{\partial w} L \tag{80} \\ \\
&amp;b_{novo} = b_{antigo} -\alpha \frac{\partial }{\partial b} L \tag{81} \\ \\
\end{align}\end{split}\]</div>
<p>Aqui, <span class="math notranslate nohighlight">\(\alpha\)</span> representa o learning rate, um hiperparâmetro que controla o tamanho do passo na atualização dos parâmetros.
Um learning rate muito alto pode causar oscilações em torno do mínimo da função custo, enquanto um learning rate muito baixo pode resultar em um processo de treinamento lento.
Caso o leitor se interesse pelas expressões acima, fica como leitura complementar a referência <a class="reference internal" href="Parte4.html#ref6"><span class="std std-ref">[6]</span></a> .</p>
<p>A autodiferenciação, como já apresentado, é uma técnica matemática que permite calcular automaticamente as derivadas e os gradientes de funções, sendo essencial no processo da otimização via uso do gradiente descendente.
Ela facilita o cálculo dos gradientes da função custo em relação a cada peso <span class="math notranslate nohighlight">\(w\)</span> e <span class="math notranslate nohighlight">\(b\)</span>, permitindo a atualização eficiente desses parâmetros.</p>
<p>Na prática, ao treinar uma RNA, começamos com pesos <span class="math notranslate nohighlight">\(w\)</span> e bias <span class="math notranslate nohighlight">\(b\)</span> inicializados aleatoriamente. Utilizamos o gradiente descendente para minimizar a função custo
<span class="math notranslate nohighlight">\(L(w,b)\)</span> . Em cada etapa do treinamento, calculamos o gradiente da função custo em relação a cada parâmetro, ajustamos esses parâmetros na direção oposta ao gradiente
(para diminuir o erro - por isso o sinal negativo), e repetimos o processo. O treinamento prossegue até que a função custo alcance um valor mínimo, indicando que a rede aprendeu a tarefa desejada de forma eficiente.</p>
<p>A imagem abaixo representa o funcionamento de uma rede neural artificial onde a arquitetura da rede é composta pela camada de entrada, camada escondida e camada de saída.
A camada escondida possui 100 neurônios artificiais enquanto as outras duas apenas um. A arquitetura da rede é um parâmetro empírico que necessita de testes, assim como alguns dos hiperparâmetros.</p>
<figure class="align-default" id="id6">
<img alt="_images/image_24.png" src="_images/image_24.png" />
<figcaption>
<p><span class="caption-text">Figura 24</span><a class="headerlink" href="#id6" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Para essa rede neural em especifico, a função custo depende da equação diferencial e das condições de contorno do problema (poço de potencial infinito unidimensional).
Logo, se trata de um problema um pouco mais complexo, uma vez que estamos de fato utilizando uma rede neural artificial para resolver uma equação diferencial, dada as condições de contorno do sistema. A essa area de intersecção entre a Física e o machine learning, se dá o nome de
PINNS (Physics Informed Neural Networks) , ou seja, “informamos” a Física do problema via função custo para a rede neural afim de que após o treinamento ela possa nos dar uma solução para a equação diferencial apresentada.</p>
<p>Não estamos interessados em tratar de equações diferenciais, nem de criarmos nenhuma rede neural artificial neste trabalho, visto que a abordagem teórica adotada nos capítulos anteriores não trata de fato de equações diferenciais, contudo, podemos trazer um exemplo em que simulamos o processo
de aprendizado de uma rede neural artificial, com o objetivo de elucidarmos o uso da autodiferenciação para minimizar uma função custo. (Entretanto, caso haja interesse pelo tema, fica a cargo do leitor o estudo das referências <a class="reference internal" href="Parte4.html#ref7"><span class="std std-ref">6 e 7</span></a> ).</p>
<p>Através de um algoritmo em Python e da biblioteca JAX, iremos minimizar uma função custo que vai depender do peso sináptico (<span class="math notranslate nohighlight">\(w\)</span> ) e do bias (<span class="math notranslate nohighlight">\(b\)</span> ).</p>
<p>Este código exemplifica um processo básico de otimização em aprendizado de máquina, onde uma função de custo é minimizada ajustando iterativamente os parâmetros (pesos) da rede.</p>
<p>Por fim, podemos apresentar o algoritmo e discorrer sobre seu significado e resultados.</p>
<p>Entrada:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Função para atualizar os pesos subtraindo o gradiente multiplicado pela taxa de aprendizado</span>
<span class="k">def</span> <span class="nf">atualizar_pesos</span><span class="p">(</span><span class="n">pesos</span><span class="p">,</span> <span class="n">gradientes</span><span class="p">,</span> <span class="n">taxa_aprendizado</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">pesos</span> <span class="o">-</span> <span class="n">taxa_aprendizado</span> <span class="o">*</span> <span class="n">gradientes</span>

<span class="c1"># Função custo quadrática</span>
<span class="k">def</span> <span class="nf">funcao_custo</span><span class="p">(</span><span class="n">pesos</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pesos</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Inicialização dos pesos</span>
<span class="n">pesos</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0</span><span class="p">])</span>

<span class="c1"># Definição da taxa de aprendizado</span>
<span class="n">taxa_aprendizado</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Preparação da função para calcular o gradiente da função de custo</span>
<span class="n">grad_funcao_custo</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">funcao_custo</span><span class="p">)</span>

<span class="c1"># Lista para armazenar os valores do custo para posterior visualização</span>
<span class="n">valores_custo</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop de treinamento para atualizar os pesos</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># Calcula o gradiente da função de custo</span>
    <span class="n">gradientes</span> <span class="o">=</span> <span class="n">grad_funcao_custo</span><span class="p">(</span><span class="n">pesos</span><span class="p">)</span>

    <span class="c1"># Atualiza os pesos</span>
    <span class="n">pesos</span> <span class="o">=</span> <span class="n">atualizar_pesos</span><span class="p">(</span><span class="n">pesos</span><span class="p">,</span> <span class="n">gradientes</span><span class="p">,</span> <span class="n">taxa_aprendizado</span><span class="p">)</span>

    <span class="c1"># Calcula e armazena o custo atual</span>
    <span class="n">custo_atual</span> <span class="o">=</span> <span class="n">funcao_custo</span><span class="p">(</span><span class="n">pesos</span><span class="p">)</span>
    <span class="n">valores_custo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">custo_atual</span><span class="p">)</span>

<span class="c1"># Plotando o gráfico do custo ao longo das iterações</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">valores_custo</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Custo em relação a epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epoch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Custo&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Saída:</p>
<figure class="align-default" id="id7">
<img alt="_images/image_25.png" src="_images/image_25.png" />
<figcaption>
<p><span class="caption-text">Figura 25</span><a class="headerlink" href="#id7" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>O código acima ilustra o processo de otimização de parâmetros (pesos) de forma simplificada, usando gradiente descendente e a biblioteca JAX para autodiferenciação.</p>
<p>Começamos importando as bibliotecas necessárias.</p>
<p>A função <strong>atualizar_pesos</strong> é definida para ajustar os pesos. Ela recebe os pesos iniciais, os gradientes desses pesos e uma taxa de aprendizado.
Os pesos são atualizados subtraindo o produto do gradiente pelo valor da taxa de aprendizado. Este é um passo fundamental no gradiente descendente, um método comum para otimização em machine learning.
A função <strong>funcao_custo</strong> calcula o custo  com base nos pesos atuais. No nosso caso, a função custo é simplesmente a soma dos quadrados dos pesos.
O objetivo é minimizar essa função custo ajustando os pesos.
Os pesos são inicializados com valores específicos, e uma taxa de aprendizado é definida. A taxa de aprendizado determina o tamanho dos passos que são
dados na direção oposta ao gradiente durante a otimização.</p>
<p>Utilizamos a função <strong>grad</strong> da biblioteca JAX, o gradiente da função custo é preparado para ser calculado automaticamente. Isso permite que o código
calcule os gradientes necessários para a atualização dos pesos sem a necessidade de derivadas manuais ou numéricas.
O algoritmo executa um loop de otimização, onde em cada iteração (epoch), os gradientes são calculados e os pesos são atualizados.
O valor da função custo é recalculado após cada atualização de peso e armazenado em listas para plotagem.
Após o loop de otimização, o código plota um gráfico mostrando como o valor da função custo diminuiu ao longo das iterações.
Este gráfico é útil para visualizar o progresso da otimização e confirmar que a função custo está de fato sendo minimizada.</p>
<p>Em resumo, o código demonstra um exemplo básico de como os pesos podem ser otimizados usando o gradiente descendente.
O foco está em mostrar como os pesos influenciam o valor da função custo e como eles podem ser ajustados iterativamente para minimizar
esse custo. Embora o exemplo seja simplificado e não esteja ligado a uma aplicação de aprendizado de máquina específica, ele fornece uma base
conceitual para entender a otimização de parâmetros em contextos mais complexos.</p>
</section>
</section>

</main>
                        <nav aria-label="Page navigation" class="py-4 my-5 clearfix font-weight-bold border-top">
    <a class="float-left" href="Parte2.html" title="Previous">
        <span aria-hidden="true">←&nbsp;</span>2. <strong>Diferenciação Numérica</strong>
    </a>
    <a class="float-right" href="Parte4.html" title="Next">
        4. <strong>Referências</strong> <span aria-hidden="true">&nbsp;→</span>
    </a>
</nav>
                    </div>
                    
                    <nav class="col-12 col-lg-3 pb-4">
                        <div class="sticky-top toc page-toc" aria-labelledby="page-toc-heading">
                            <p class="font-weight-bold" id="page-toc-heading">Page contents</p>
                            <ul>
<li><a class="reference internal" href="#">3. <strong>Diferenciação Automática</strong></a><ul>
<li><a class="reference internal" href="#motivacao">3.1. <strong>Motivação</strong></a></li>
<li><a class="reference internal" href="#numeros-duais">3.2. <strong>Números duais</strong></a></li>
<li><a class="reference internal" href="#implementando-a-autodiferenciacao">3.3. <strong>Implementando a autodiferenciação</strong></a></li>
<li><a class="reference internal" href="#modo-direto-e-reverso">3.4. <strong>Modo direto e reverso</strong></a></li>
<li><a class="reference internal" href="#bibliotecas-de-diferenciacao-automatica">3.5. <strong>Bibliotecas de diferenciação automática</strong></a><ul>
<li><a class="reference internal" href="#jax">3.5.1. <strong>JAX</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#aplicacao-em-redes-neurais-artificiais">3.6. <strong>Aplicação em Redes Neurais Artificiais</strong></a></li>
</ul>
</li>
</ul>

                        </div>
                    </nav>
                    
                </div>
            </div>
        </div>
    </div>
    <footer class="container-fluid bg-primary text-light">
        <div class="container">
            <div class="row">
        <div class="col p-4">
            
                <nav aria-label="Footer">
                    <ul class="nav justify-content-center mb-2">
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/features/">Features</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/about-wagtail/"> About Wagtail</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/services/"> Services</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/blog/"> Blog</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/packages/"> Packages</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/developers/"> Developers</a></li>
                        
                    </ul>
                </nav>
            
            <div class="text-center">
                <p style="display: none">
                    <a class="text-light" href="https://github.com/wagtail/sphinx_wagtail_theme" rel="nofollow" target="_blank">
                        Wagtail Sphinx Theme 6.1.1
                    </a>
                </p>
            </div>
            <div class="text-center">
                    &copy; Copyright 2023, Sphinx
            </div>
        </div>
    </div>
        </div>
    </footer>
        <script src="_static/documentation_options.js?v=29a6c3e3"></script>
        <script src="_static/doctools.js?v=888ff710"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script type="text/javascript" src="_static/dist/theme.js"></script>
        <script type="text/javascript" src="_static/dist/vendor.js"></script>
        <script type="text/javascript" src="_static/searchtools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script type="text/javascript">
            document.addEventListener('DOMContentLoaded', function() { Search.loadIndex("searchindex.js"); });
        </script>
        <script type="text/javascript" id="searchindexloader"></script>
    </body>
</html>